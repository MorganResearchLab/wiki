{"searchDocs":[{"title":"Model","type":0,"sectionRef":"#","url":"/wiki/common-resources/model","content":"Model","keywords":"","version":"Next"},{"title":"Morgan Lab Shared Folder","type":0,"sectionRef":"#","url":"/wiki/common-resources/morgan-lab-shared-folder","content":"Morgan Lab Shared Folder","keywords":"","version":"Next"},{"title":"Contributing","type":0,"sectionRef":"#","url":"/wiki/contributing","content":"Contributing","keywords":"","version":"Next"},{"title":"Preprocessing","type":0,"sectionRef":"#","url":"/wiki/common-resources/scrna-seq/preprocessing","content":"Preprocessing","keywords":"","version":"Next"},{"title":"Sample header","type":0,"sectionRef":"#","url":"/wiki/software/sample-header","content":"Sample header","keywords":"","version":"Next"},{"title":"Managing Files and Directories","type":0,"sectionRef":"#","url":"/wiki/working-with-hpc/managing-files-and-directories","content":"Managing Files and Directories As a member of the lab, you will have access to three different storage resources: home directory (~ or uoa/home/{your_user_id})scratch directory (~/sharedscratch or /uoa/home/{your_user_id}/sharedscratch)Morgan_Lab shared folder (/uoa/scratch/shared/Morgan_Lab) The following breakdown shows the storage limit and file limit for each resource: id\tName\tStorage Limit\tFile Limit0\thome\t64G\t200K 1\tscratch\t1T\t200K 2\tMorgan_Lab\t1T\t200K (In reality, there are no definitive limits to the storage size. However, it's recommended to keep your scratch directory at around 1T unless you have exceptional circumstances, as the HPC admins will monitor them from time to time.) The home and scratch directories are personal directories, and no one else is supposed to have access to them except you. Most of the time, all files are supposed to be stored in the scratch directory. The home directory is only meant to store some configurations (hidden directories that start with .), and some small files from your prototyping may go there. The Morgan_Lab shared folder is shared among the members of the group. If you're not part of the group, you will have to raise a ticket to the IT helpdesk here.","keywords":"","version":"Next"},{"title":"Frequently asked questions","type":0,"sectionRef":"#","url":"/wiki/working-with-hpc/frequenty-asked-questions","content":"","keywords":"","version":"Next"},{"title":"How do I check my HPC usage?​","type":1,"pageTitle":"Frequently asked questions","url":"/wiki/working-with-hpc/frequenty-asked-questions#how-do-i-check-my-hpc-usage","content":" $ sreport cluster UserUtilizationByAccount users=r04mr23 start=2000-01-01 format=Accounts%20,Cluster%16,Login,Used -------------------------------------------------------------------------------- Cluster/User/Account Utilization 2000-01-01T00:00:00 - 2024-06-22T23:59:59 (772412400 secs) Usage reported in CPU Minutes -------------------------------------------------------------------------------- Account Cluster Login Used -------------------- ---------------- --------- -------- mmsn maxwell_cluster r04mr23 281760   Change the r04mr23 with your Maxwell username, it's in minutes, so you have to divide it by 60.  ","version":"Next","tagName":"h2"},{"title":"I set my #SBATCH --time correctly but my job is being stopped mid way?​","type":1,"pageTitle":"Frequently asked questions","url":"/wiki/working-with-hpc/frequenty-asked-questions#i-set-my-sbatch---time-correctly-but-my-job-is-being-stopped-mid-way","content":" The reason might be because you didn't set the partition in the first place and your job being assigned to spot-compute which can only be used for running short job no longer than 36 hours.Add the #SBATCH --partition uoa-compute to run job that is longer than 36 hours  ","version":"Next","tagName":"h2"},{"title":"How do I check how much storage have I used? du -sh seems to take forever​","type":1,"pageTitle":"Frequently asked questions","url":"/wiki/working-with-hpc/frequenty-asked-questions#how-do-i-check-how-much-storage-have-i-used-du--sh-seems-to-take-forever","content":" Chek home (/uoa/home/r04mr23) directory usage  $ beegfs-ctl --getquota --cfgFile=/etc/beegfs/beegfs-client-uoa-home.conf --uid r04mr23 Quota information for storage pool Default (ID: 1): user/group || size || chunk files name | id || used | hard || used | hard --------------|------||------------|------------||---------|--------- r04mr23|193544|| 14.19 GiB| 56.00 GiB|| 75776| 200000   Check sharedscratch directory (/uoa/home/r04mr23/sharedscratch) usage  $ beegfs-ctl --getquota --cfgFile=/etc/beegfs/beegfs-client-uoa-scratch.conf --uid r04mr23 user/group || size || chunk files name | id || used | hard || used | hard --------------|------||------------|------------||---------|--------- r04mr23|193544|| 2.52 TiB| 1024.00 GiB|| 1002345| 1000000   Again change r04mr23 with your Maxwell username, size used is your usage, size hard is the non-official storage limit, try to keep it below hard as suggested by the Maxwell team. chunk files is the file number limit. ","version":"Next","tagName":"h2"},{"title":"Protip","type":0,"sectionRef":"#","url":"/wiki/protip","content":"","keywords":"","version":"Next"},{"title":"Terminal autocompletion​","type":1,"pageTitle":"Protip","url":"/wiki/protip#terminal-autocompletion","content":" Tired of going through of your bash history and pressing up arrow key to find that one specific command? Use bash autocompletion  Create a file called .inputrc in your home directory (e.g., /uoa/home/r04mr23/.inputrc)Copy and paste the following to the file .inputrc &quot;\\e[A&quot;: history-search-backward &quot;\\e[B&quot;: history-search-forward Save it and reload your terminal by creating a new terminal (it's the + button beside the bash)Now whenever you're typing a command you can use up or down arrow key to search for the previous command that you used. Here's the example: # I typed `sba` [r04mr23@maxlogin1(maxwell) ~]$ sba # I pressed up arrow [r04mr23@maxlogin1(maxwell) ~]$ sbatch /uoa/home/r04mr23/sharedscratch/src/start_jupyter.sh benccchmarker-simulation # I pressed the up arrow again [r04mr23@maxlogin1(maxwell) ~]$ sbatch alignment_slurm.sh   ","version":"Next","tagName":"h2"},{"title":"Passwordless Auth to VS Code Remote​","type":1,"pageTitle":"Protip","url":"/wiki/protip#passwordless-auth-to-vs-code-remote","content":" It can be annoying to have to reenter your Maxwell password every time you're trying to connect to it. Here's how you can bypass it:  Open terminal on your local pcGenerate public and private keys using OpenSSL, change &lt;local_user&gt; with your username (e.g., r04mr23 for me) ssh-keygen -q -b 2048 -P &quot;&quot; -f /Users/&lt;local_user&gt;/.ssh/keys/maxlogin_rsa -t rsa This will output two files maxlogin1_rsa (private key) and maxlogin1_rsa.pub (publick key)Update your ssh config located on /Users/&lt;localuser&gt;/.ssh/config (create one if you don't have) by adding the following, again change the &lt;local_user&gt; and &lt;remote_user&gt; with yours, usually they're the same for Maxwell Host maxlogin1 HostName maxlogin1.abdn.ac.uk User &lt;remote_user&gt; Port 22 PreferredAuthentications publickey IdentityFile &quot;/Users/&lt;local_user&gt;/.ssh/keys/maxlogin1_rsa&quot; Copy the content of the maxlogin1_rsa.pub to &lt;remote_user_home_dir&gt;/.ssh/authorized_keys (in Maxwell it's /uoa/home/&lt;remote_user_name&gt;/.ssh/authorized_keys create one if it doesn't exist), your file should look like this now ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCxu/dOSNX1aRYB8Abl1Jbj0zFTwCCQBaXi/ZsWJS4mEX0RkXckItf2hf0O14PBZ8DdW9RS/zdadadasdaystdeyuasgdhjagjhdgashjdghjasdtyaatyuetyquegjhwqghjdaghjsdghsjagdhjagsdhjaghjdgahsjdghjastdyaudtyuatdyuatsyudtasyudtasyudryrqtyertyqwretfqghwefqgwhefghqfhegqfwgheqw m.ramdhani.23@abdn.ac.uk Update the permission to 600 (File can only be accessed by you, inaccessible to everyone else) [r04mr23@maxlogin1(maxwell) ~]$ chmod 600 /uoa/home/&lt;remote_user_name&gt;/.ssh/authorized_keys Now you can log in to VS Code without password! ","version":"Next","tagName":"h2"},{"title":"Getting started","type":0,"sectionRef":"#","url":"/wiki/working-with-hpc/getting-an-account","content":"","keywords":"","version":"Next"},{"title":"Getting an account​","type":1,"pageTitle":"Getting started","url":"/wiki/working-with-hpc/getting-an-account#getting-an-account","content":" To connect and log in to the HPC (referred to as Maxwell), you need to set up an account.  Fill out this form. The fields should be self-explanatory.Wait for an email confirming that your account is ready. This usually takes 2-5 days.  ","version":"Next","tagName":"h2"},{"title":"Installing VPN​","type":1,"pageTitle":"Getting started","url":"/wiki/working-with-hpc/getting-an-account#installing-vpn","content":" If you want to connect to Maxwell and you're not using the university network, you may need to install a VPN.  ","version":"Next","tagName":"h2"},{"title":"Macbook​","type":1,"pageTitle":"Getting started","url":"/wiki/working-with-hpc/getting-an-account#macbook","content":" Click the bee-like icon beside the Bluetooth icon on the menu bar.Select Business.Install UoA Remote Access.  ","version":"Next","tagName":"h3"},{"title":"Non uni-managed devices​","type":1,"pageTitle":"Getting started","url":"/wiki/working-with-hpc/getting-an-account#non-uni-managed-devices","content":" If you're using a device not managed by the university, you may not have access to the F5 VPN. However, you can use the Web VPN.  Go to remote.abdn.ac.uk.Log in with your university account.Click either maxlogin1 or maxlogin2 (These are your login nodes)When the pop-up appears click start Another pop-up will appear and select Open F5 Networks VPNOnce you're connected, you will see the F5 VPN on the system tray  ","version":"Next","tagName":"h3"},{"title":"Installing Git Bash​","type":1,"pageTitle":"Getting started","url":"/wiki/working-with-hpc/getting-an-account#installing-git-bash","content":" You may need Git Bash if you're not using a UNIX system (e.g., Windows but newer Windows has Powershell built in so you should be fine, but just in case)  ","version":"Next","tagName":"h2"},{"title":"Windows​","type":1,"pageTitle":"Getting started","url":"/wiki/working-with-hpc/getting-an-account#windows","content":" Download git standalone installer from here Make sure you're downloading the correct build based on your Windows system architecture (32/64-bit) Install it ","version":"Next","tagName":"h3"},{"title":"Slurm interactive session","type":0,"sectionRef":"#","url":"/wiki/working-with-hpc/slurm-interactive-session","content":"Slurm interactive session One of the common beginner mistake when prototyping or running something on the HPC is to run the job on the login node, which technically should be fine if you're not running something that requires intensive resources. This is where Slurm interactive session comes into play. With Slurm interactive session we can run our programs within HPC clusters in real-time. So instead of using login node with the limited resource that it has, we can utilise the nodes in a full shell session. To start an interactive session you can use the srun command with the following arguments --nodes: Specify the number of compute node that you want to use--ntasks-per-node: Tell the number of tasks we want to run per node--time: Set how long we want the interactive session to last in DD-hh:mm:ss format--pty: Create a pseudoterminal for us to interact with the HPCbash: Use bash interactive as the terminal-i: Let bash accept the standard input (keyboard input) [r04mr23@maxlogin1(maxwell) ~]$ srun --nodes=1 --ntasks-per-node=1 --time=01:00:00 --pty bash -i Running the above command will let us enter the interactive session, you will notice how the hostname changes from maxlogin1 to one of the node hostname in the HPC, in my case it's vhmem002 [r04mr23@vhmem002(maxwell) ~]$ To exit the interactive session and going back to the login node you can use the exit command. Happy hacking!","keywords":"","version":"Next"},{"title":"Managing packages","type":0,"sectionRef":"#","url":"/wiki/working-with-hpc/managing-packages","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#introduction","content":" There are many reasons why we need to use package manager or environment over installing packages globally (which most likely you won't be able to do as you don't have the sudo access), here are some:  Dependency Management By using package manager/environment makes it easier to ensure that all the dependencies we use when we're developing our software consistent as it helps manage the different version of libraries/packages which prevents compatibility issues or version conflicts to happenIsolation Each project can have its own set of dependencies isolated from other projects. This is particularly important when different projects require different versions of the same library.Reproducibility By using environment or package managers, you can recreate the exact environment needed to run an application, ensuring that it works the same way on different machines.  And honestly many other reasons why but hopefully you should be convinced to use package manager by this point.  There are different ways on how you can manage your packages/environments and it also depends on the language that you're using as your development tools. However, since we're talking about Maxwell and you're most likely to use Python or R, there are three different package managers that I used most often, they are venv, conda and mamba. Here are the simple comparison between them  id\tname\tadvantages\tdisadvantages0\tvenv\t- Installing with pip is blazingly fast - Most of the python packages are in pip\t- Can only manage Python libraries 1\tconda\t- Can manage not only Python (e.g., R, Julia etc.)\t- Installation is slow compared to PIP 2\tmamba\t- Basically a conda wrapper with faster installation\t- None that I have found yet  ","version":"Next","tagName":"h2"},{"title":"venv​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#venv","content":" ","version":"Next","tagName":"h2"},{"title":"Creating a virtual environment​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#creating-a-virtual-environment","content":" [r04mr23@maxlogin1(maxwell) ~]$ python -m venv my_env   This will create a virtual environment called my_env located in the directory where you run the command, in my case it's the ~/home directory  ","version":"Next","tagName":"h3"},{"title":"Activating the environment​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#activating-the-environment","content":" [r04mr23@maxlogin1(maxwell) ~]$ source my_env/bin/activate # (my_env) will appear at the start if you successfully activated the environment (my_env) [r04mr23@maxlogin1(maxwell) ~]$   If you check the Python using which python it will give you the path to the binary inside of the virtual environment  ","version":"Next","tagName":"h3"},{"title":"Managing packages​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#managing-packages-1","content":" You can use pip to install, update or remove packages packages, for example to install pandas  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ pip install pandas   If you installed it successfully, now if you check the list of installed packages, you will see pandas in it  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ pip list Package Version --------------- ----------- numpy 2.0.0 pandas 2.2.2 pip 23.0.1 python-dateutil 2.9.0.post0 pytz 2024.1 setuptools 65.5.0 six 1.16.0 tzdata 2024.1   To install specific version of a package  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ pip install pandas=1.2.3   To update a package  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ pip install pandas --upgrade   OR  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ pip install pandas -U   To uninstall a package  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ pip uninstall pandas   ","version":"Next","tagName":"h3"},{"title":"Saving installed packages version for reproducibility​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#saving-installed-packages-version-for-reproducibility","content":" You can save the installed packages in your environment using pip freeze &gt; requirements.txt for which you can then use the requirements.txt to install similar packages and version on a new environment  requirements.txt numpy==2.0.0 pandas==2.2.2 python-dateutil==2.9.0.post0 pytz==2024.1 six==1.16.0 tzdata==2024.1   ","version":"Next","tagName":"h3"},{"title":"Deactivating environment​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#deactivating-environment","content":" Once you're done with everything or when you want to switch to another environment you can use deactivate command and it will deactivate the current environment  ","version":"Next","tagName":"h3"},{"title":"More resources​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#more-resources","content":" Python Docs' venv — Creation of virtual environmentsReal Python's Python Virtual Environments: A Primerpip commands  ","version":"Next","tagName":"h3"},{"title":"Mamba​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#mamba","content":" I will skip conda because the commands will be more or less similar to mamba's  ","version":"Next","tagName":"h2"},{"title":"Creating a mamba environment​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#creating-a-mamba-environment","content":" Load mamba  [r04mr23@maxlogin1(maxwell) ~]$ module load mamba   Creating an environment called my_env  [r04mr23@maxlogin1(maxwell) ~]$ mamba create --name my_env   The coolest thing about conda/mamba is you can specify the Python version that you want, for example to create a mamba environment with Python 3.11  [r04mr23@maxlogin1(maxwell) ~]$ mamba create --name my_env python=3.11   ","version":"Next","tagName":"h3"},{"title":"Activating the environment​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#activating-the-environment-1","content":" [r04mr23@maxlogin1(maxwell) ~]$ mamba activate my_env # (my_env) will appear at the start if you successfully activated the environment (my_env) [r04mr23@maxlogin1(maxwell) ~]$   ","version":"Next","tagName":"h3"},{"title":"Managing packages​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#managing-packages-2","content":" To install pandas  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ mamba install pandas   If you installed it successfully, now if you check the list of installed packages, you will see pandas in it  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ mamba list # packages in environment at /uoa/home/r04mr23/sharedscratch/.conda/envs/my_env: # # Name Version Build Channel _libgcc_mutex 0.1 conda_forge conda-forge _openmp_mutex 4.5 2_gnu conda-forge pandas 1.2.3 conda_forge conda-forge   To install specific version of a package  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ mamba install pandas=1.2.3   To update a pakacge  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ mamba update pandas   To uninstall a package  (my_env) [r04mr23@maxlogin1(maxwell) ~]$ mamba remove pandas   ","version":"Next","tagName":"h3"},{"title":"Saving installed packages version for reproducibility​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#saving-installed-packages-version-for-reproducibility-1","content":" You can save the installed packages in your environment using conda env export -n my_env -f my_env.yaml for which you can then use the my_env.yaml to install similar packages and version on a new environment  ","version":"Next","tagName":"h3"},{"title":"Deactivating environment​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#deactivating-environment-1","content":" Once you're done with everything or when you want to switch to another environment you can use mamba deactivate command and it will deactivate the current environment  ","version":"Next","tagName":"h3"},{"title":"More resources​","type":1,"pageTitle":"Managing packages","url":"/wiki/working-with-hpc/managing-packages#more-resources-1","content":" Conda's common tasks ","version":"Next","tagName":"h3"},{"title":"Running a Jupyter Server","type":0,"sectionRef":"#","url":"/wiki/working-with-hpc/running-a-jupyter-server","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Running a Jupyter Server","url":"/wiki/working-with-hpc/running-a-jupyter-server#introduction","content":" Jupyter notebook is very handy for fast-prototyping or quick Python code testing. In this guide, we will be using VS Code to create a Jupyter Server and run Jupyter notebook on Maxwell. Before starting please make sure you have installed the following:  Visual Studio CodeVS Code Jupyter extension  Every file with .ipynb extension will be detected as a Jupyter notebook file, you can create a file and put a .ipynb extension and it will become a Jupyter notebook  To start working with Jupyter notebook you will need to select a kernel   There are two options, you can use the readily available environments (Maxwell default conda environments, your conda environments or your pip environments if you're in the same directory with it). The problem with selecting this option is the fact that you will be using login node which is very slow and may fail because of you don't have enough memory to run your script. Most of the time we will opt for the second choice, to use the Existing Jupyter server.  ","version":"Next","tagName":"h2"},{"title":"Creating Jupyter server​","type":1,"pageTitle":"Running a Jupyter Server","url":"/wiki/working-with-hpc/running-a-jupyter-server#creating-jupyter-server","content":" To create a Jupyter server we need to submit a slurm job that handles the running of Jupyter server in the background for us  start_jupyter.sh #!/bin/bash #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --ntasks=1 #SBATCH --mem=64G #SBATCH --job-name=jupyter-notebook #SBATCH --output=jupyter-notebook.log module load mamba source ~/.bash_profile source ~/.bashrc mamba activate my_environment jupyter notebook --no-browser --ip=0.0.0.0   Change #SBATCH options according to your need, in my case I need 64 GB memory, with 8 CPUs per task and keep it running for the next 1 hour. If you're not sure with the #SBATCH option or you want to add more option, you can refer to this. The next lines I'm loading mamba, my bash_profile and setup (The reason was if I remember it correctly because if I don't do it it will use the default base environment that may cause clash with the package dependencies installed in my environment, so yes it's better to include them). The lines that follow is to activate my environment (NOTE: You must have ipykernel package installed in your environment otherwise the server creation will fail) and to create a Jupyter notebook server.  Once you submit the job, the log will appear (in my case jupyter-notebook.log) wait for around 2 or 3 minutes till the log file is filled with Jupyter logs (Either server is fully running or it's failed). Open the jupyter-notebook.log, if it's successful you will see the url to the server.    Copy the url, the one that doesn't start with the http://127.0.0.1...... (in my case it's http://vhmem002.int.maxwell.abdn.ac.uk:8888/tree?token=fac698f887a93963b6b7f860158e2e83ce5f03b4281978e6)  Go back to your Jupyter notebook, click Select Kernel and select Existing Jupyter Server..., paste the url, press enter and when prompted with environment selection, select your environment (There is only one of it).  ","version":"Next","tagName":"h2"},{"title":"More resources​","type":1,"pageTitle":"Running a Jupyter Server","url":"/wiki/working-with-hpc/running-a-jupyter-server#more-resources","content":" Visual Studio Code's Jupyter Notebooks in VS Code ","version":"Next","tagName":"h2"},{"title":"Running a job","type":0,"sectionRef":"#","url":"/wiki/working-with-hpc/running-a-job","content":"Running a job For this tutorial I'll be creating an empty folder called tutorial in my sharedscratch The VS Code way Right click on the sharedscratch folder and select New Folder...Name the folder tutorial Shell $ mkdir sharedscratch/tutorial Let's say I want to run a Python script called test.py on Maxwell, it's just a simple script that will print This is a Maxwell test 100 times # sharedscratchtutorial/tutorial/test.py for i in range(100): print(&quot;This is a Maxwell test&quot;) The VS Code way Create a new file by right clicking on the tutorial folder Name the file test.py Copy and paste the code above to the file Save the file by pressing CTRL+S on Windows or ⌘+ s on Mac You can open a terminal CTRL + ` on Windows or ⌃ + ` on Mac, then run the code by typing python test.py and should be working smoothly for something small like this, but in reality you'll be working with much bigger stuff and its not recommended to run your whole pipeline on a login node, so instead of doing that we will use SLURM Create a new file called slurm.sh in the same directory as your test.py Paste the following blocks of code to the slurm.sh #!/bin/bash #SBATCH --job-name=tester #SBATCH -o tester.out #SBATCH --ntasks=1 #SBATCH --mail-type=begin #SBATCH --mail-type=end #SBATCH --mail-user=m.ramdhani.23@abdn.ac.uk python test.py The first line is called the shebang it specifify the interpreter to be used to run the code, next lines starting with #SBATCH are the special instructions we'll use to interact with the Slurm Workload Manager --job-name: Name for the job--o: Output log--ntasks: Number of tasks to be executed in parallel--mail-..: Send a mail when the job starts running and when the job is finished to --mail-user email address Once we're set we can run the code from the terminal # Change the directory if you haven't done so $ cd sharedscratch/tutorial # Run the script $ sbatch slurm.sh You can check the status of the job by using squeue -u &lt;YOUR_USER_ID&gt; You can also see the log once your job starts running in a file called tester.out Shell $ cd sharedscratch/tutorial $ vim test.py esc then i to insert text, ctrl + v to paste the python code, save it by pressing esc then w then q and enter to save file vim slurm.sh Do the same thing above but this time paste the code for slurm.sh and save it $ sbatch slurm.sh $ squeue -u &lt;YOUR_USERNAME&gt; When running your job you might feel like you want to update your script or stop the already running job to do this you can use scancel &lt;JOB_ID&gt; JOB_ID of your job can be found on the first column when you run squeue -u &lt;YOUR_USERNAME&gt; More resources: Bioinformatics Workbook's Quick Reference Sheet for HPCUSC's HPC CheatsheetSlurm Workload Manager Documentation","keywords":"","version":"Next"},{"title":"Using Maxwell","type":0,"sectionRef":"#","url":"/wiki/working-with-hpc/using-maxwell","content":"","keywords":"","version":"Next"},{"title":"Command line/Terminal/Command Prompt/Powershell​","type":1,"pageTitle":"Using Maxwell","url":"/wiki/working-with-hpc/using-maxwell#command-lineterminalcommand-promptpowershell","content":" Windows: Type cmd or powershell on the explorer near the start bar.Mac: ⌘+ space, then type terminalLinux: ctrl+alt+t  You can use ssh to connect to Maxwell, there are two login nodes that you can use maxlogin1.abdn.ac.uk and maxlogin2.abdn.ac.uk. You can use your id (For example my ID is r04mr23) and password to connect to Maxwell.  ssh r04mr23@maxlogin1.abdn.ac.uk   It may show you this message  The authenticity of host 'maxlogin1.abdn.ac.uk' can't be established. ECDSA key fingerprint is SHA256:CwrcHjdd9349u38rj392fr9j389rj3298rj23. Are you sure you want to continue connecting (yes/no/[fingerpint])   For now to not complicate things just type yes and you're good to go!  ","version":"Next","tagName":"h2"},{"title":"First time using HPC/Terminal/Command line?​","type":1,"pageTitle":"Using Maxwell","url":"/wiki/working-with-hpc/using-maxwell#first-time-using-hpcterminalcommand-line","content":" If you're using HPC/Terminal/Command line for the first time, here are some guides that might be useful for you:  Software Carpentry's Intro to Unix ShellCarpentries Incubator's Intro to HPCAlexji's UNIX cheatsheetBioinformatics Workbook's UNIX basics  There are probably more tutorials that better suit your liking if you Google and look for unix tutorial site:github.io  ","version":"Next","tagName":"h3"},{"title":"VS Code Remote SSH Extension​","type":1,"pageTitle":"Using Maxwell","url":"/wiki/working-with-hpc/using-maxwell#vs-code-remote-ssh-extension","content":" Terminal is powerful but sometimes you just want to prototype things faster and more efficient (which honestly if you're some sort of UNIX gods you'd probably be able to do it but I'm not) in more user-friendly environment, not to mention the painful process of transferring files using scp and sftps, well technically you can use FileZilla or PUTTY for it but why bother switching back and forth between two softwares when you can have all them in one. VS Code got your back, it offers a graphical user interface (GUI) that lets you edit code, run a terminal, and leverage thousands of extensions to streamline development. To use VS code;  Download VS Code (Don't confuse it with Microsoft's other product called Visual Studio)Install it (of course)Open it and click the Extension icon on the left side paneSearch for Remote Development extension pack and install it, this will install all of the required extensions for SSH VS Code remote developmentOnce you have it installed you will have the Remote Explorer option under the Extension icon, click itHover over SSH and Click the New Remote (+) buttonType the ssh login, press enter (Change r04mr23 with your id, because it's mine) ssh r04mr23@maxlogin1.abdn.ac.uk It will ask you what kind of operating system, select LinuxNow when you click the Explorer icon on the left side pane (or ⇧⌘E), you can select which directory you want to open⌃ + ``` will open the terminal for you  Happy exploring and becoming more productive. More on VS Code and the Remote Explorer extension  Microsoft's Visual Studio Code BasicsMicrosoft's Remote Development using VS Code  ","version":"Next","tagName":"h2"},{"title":"Other resources​","type":1,"pageTitle":"Using Maxwell","url":"/wiki/working-with-hpc/using-maxwell#other-resources","content":" Aiden Durrant's Machine Learning Guide for UoA HPC ClustersMaxwell's User Manual ","version":"Next","tagName":"h2"},{"title":"Working with R","type":0,"sectionRef":"#","url":"/wiki/working-with-hpc/working-with-r","content":"Working with R From time to time you may be required to work with R script as most of the bioinformatics/computational biology packages are written in R (need citation). In your local computer, working with R may appear fairly easy as you can always install RStudio Desktop and execute your R script there. But what if you want to run a script that requires high memory and your computer doesn't have enough RAM? Yes you can always rely on Maxwell! It's pretty straightforward to work with R on HPC as you can just create a ready-to-run R script and run it using the Rscript command. Rscript myscript.R With VS Code you can do more than just running the script, you can also enter an interactive mode that will ease development and analysis (basically like using RStudio but on Maxwell and with powerful tooling that VS Code extension offers). So here's how we can do it: Open your terminal (ctrl + `) and create a new conda/mamba environment and activate it [r04mr23@maxlogin1(maxwell) ~]$ mamba create -n myrenv [r04mr23@maxlogin1(maxwell) ~]$ mamba activate myrenv (myrenv) [r04mr23@maxlogin1(maxwell) ~]$ Install the R and R Debugger extensions for VS Code In order for VS Code R to work you have to install the following packages Mandatory: R: The R programming language itselfr-httpgd: Required for VS Code interactive plot viewer to workradian: A better R terminalr-languageserver: Required for VS Code to provide code completion, dignostics, formatting and any more featuresr-jsonlite: Relatively fast jsonparser for statistical data and the webr-irkernel: R Kernel for Jupyter Not mandatory but will be used for this guide:r-ggplot2: Data visualisationr-dplyr: Data manipulation (myrenv) [r04mr23@maxlogin1(maxwell) ~]$ mamba install R radian r-httpgd r-lang r-jsonlite r-languageserver r-irkernel r-ggplot2 r-dplyr Open the settings.json by pressing command + , or Code -&gt; Settings... -&gt; Setting and click the icon pointed below Update settings.json file by adding the following parameters &quot;r.rterm.linux&quot;: &quot;/uoa/home/r04mr23/sharedscratch/.conda/envs/myrenv/bin/radian&quot;, &quot;r.alwaysUseActiveTerminal&quot;: true, &quot;r.sessionWatcher&quot;: true, &quot;r.rpath.linux&quot;: &quot;/uoa/scratch/users/r04mr23/.conda/envs/myrenv/bin/R&quot; Replace r.term.linux and r.rpath.linux with the path where R and radian installed in your environment. If you're not sure, in the environment that you activated check using the following (myrenv) [r04mr23@maxlogin1(maxwell) ~]$ which radian | xargs readlink -f /uoa/scratch/users/r04mr23/.conda/envs/myrenv/bin/radian (myrenv) [r04mr23@maxlogin1(maxwell) ~]$ which R | xargs readlink -f /uoa/scratch/users/r04mr23/.conda/envs/myrenv/bin/R Copy the path, update your settings.json, save it (ctrl/command + s) and close it Turn on the session watcher this will allow the communication between VS Code R and R Live session by performing the following commands (Read more on here) Edit the .Rprofile on your home directory (myrenv) [r04mr23@maxlogin1(maxwell) ~]$ vim ~/.Rprofile Append the following code to it if (interactive() &amp;&amp; Sys.getenv(&quot;RSTUDIO&quot;) == &quot;&quot;) { source(file.path(Sys.getenv(if (.Platform$OS.type == &quot;windows&quot;) &quot;USERPROFILE&quot; else &quot;HOME&quot;), &quot;.vscode-R&quot;, &quot;init.R&quot;)) } Reload your terminal In the directory of your choice create a new R script test.R and copy the code below test.R print(&quot;Test Maxwell R script&quot;) x &lt;- &quot;Tester&quot; Launch the terminal if you haven't (ctrl + `) and run radian (myrenv) [r04mr23@maxlogin1(maxwell) ~]$ radian R version 4.3.3 (2024-02-29) -- &quot;Angel Food Cake&quot; Platform: x86_64-conda-linux-gnu (64-bit) r$&gt; Attach the terminal to vscode-R to the current session (this will allow you to keep track of variables that you created during the R-session etc.) r$&gt; .vsc.attach() If successful it will change from the R: (not attached) to R: whatever version you're using Now you can see the R: workspace (namespaces, variables etc) on the EXPLORER tabYou can customise the layout by dragging the workspace around VSCode R allows you to preview the dataset that you loaded, let's say I'm loading a data frame variable called midwest from this ggplot tutorial midwest &lt;- read.csv(&quot;http://goo.gl/G1K41K&quot;) After you run it, the midwest variable will appear in the Workspace, clicking the magnifier icon will open the data.frame variable for us With VSCode R we can also preview plots, for example the following will generate a scatter plot using ggplot2 library(ggplot2) ggplot(midwest, aes(x=area, y=poptotal)) + geom_point() That's how we can use Maxwell and VSCode to create an interactive R. However we still have a problem, we're running this R session in a login node! r$&gt; Sys.info()[&quot;nodename&quot;] nodename &quot;maxlogin1.int.maxwell.abdn.ac.uk&quot; Which isn't recommended as it has limited memory and may affect the login node performance. In order to run R in the other compute nodes we can create an interactive session and then run radian/R terminal in it. Happy hacking!","keywords":"","version":"Next"},{"title":"Singularity on Mac, Reproducible Research and Lesson Learned","type":0,"sectionRef":"#","url":"/wiki/misc/singularity-on-mac","content":"Singularity on Mac, Reproducible Research and Lesson Learned Remember the time when you want to install multiple software to reproduce someone else's research? What does it remind you of? Pain? Exhaustion from hours of Googling and troubleshooting? Only to face the famous &quot;version compatibility&quot; issues even after you're sure everything right according to the tutorial. Ah, the joys of science and its ever-looming &quot;reproducibility crisis&quot; [[schooler_2014]]! Luckily we have containerisation technology these days! As written on the IBM website &quot;Containerisation is the packaging of software code with just the operating system (OS) libraries and dependencies required to run the code to create a single lightweight executable—called a container—that runs consistently on any infrastructure.&quot;. Started as a way to make the deployment process easier, containerisation has emerged as a valuable tool for ensuring reproducible research, so instead of letting the user install the software themselves and make them find the scripts (stored don't know where), giving them this so called 'container' streamline this process. Docker and Singularity are among the most popular containerisation platform (Well technically Docker is more popular, haven't heard of Singularity till my PI mentioned it). However since I'm largely working with HPC environments, Singularity is preferred as it's designed for such purpose and more importantly it's open-source (Long live open-source!). Singularity is built for Linux, so you have another problem if you're using Windows or Mac, in which you have to use virtual machine (VM) to run it. In this post, I'm going to share my experience dealing with Singularity on my Mac, from installing to building a singularity image file (.sif, end-product, the thing that you share to other people for reproducible research) and the lessons that I learned. So let's dive in. As I said, since Singularity is primarily created for Linux, Mac users need to use virtual machines. Several options exist to create virtual machines on Mac, including Vagrant, Lima OrbStack (which unfortunately is not very open-source). In my case I'm using Lima as I found it to be straightforward and aligned with the Singularity installation guide. Download Homebrew if you haven't had it installed $ /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; $ (echo; echo 'eval &quot;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)&quot;') &gt;&gt; $HOME/.profile $ eval &quot;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)&quot; Install Lima $ brew install lima Running lima is pretty straightforward and it comes with various distros that you can use. In this case we're going to use the default singularity-ce.yml template that is provided in the docs. # SingularityCE on Alma Linux 9 # # Usage: # # $ limactl start ./singularity-ce.yml # $ limactl shell singularity-ce singularity run library://alpine images: - location: &quot;https://repo.almalinux.org/almalinux/9/cloud/x86_64/images/AlmaLinux-9-GenericCloud-latest.x86_64.qcow2&quot; arch: &quot;x86_64&quot; - location: &quot;https://repo.almalinux.org/almalinux/9/cloud/aarch64/images/AlmaLinux-9-GenericCloud-latest.aarch64.qcow2&quot; arch: &quot;aarch64&quot; mounts: - location: &quot;~&quot; - location: &quot;/tmp/lima&quot; containerd: system: false user: false provision: - mode: system script: | #!/bin/bash set -eux -o pipefail dnf -y install --enablerepo=epel singularity-ce squashfs-tools-ng probes: - script: | #!/bin/bash set -eux -o pipefail if ! timeout 30s bash -c &quot;until command -v singularity &gt;/dev/null 2&gt;&amp;1; do sleep 3; done&quot;; then echo &gt;&amp;2 &quot;singularity is not installed yet&quot; exit 1 fi hint: See &quot;/var/log/cloud-init-output.log&quot; in the guest message: | To run `singularity` inside your lima VM: $ limactl shell {{.Name}} singularity run library://alpine TLDR; It builds an AlmaLinux and Singularity installed with it You can update the template above to include more options, for example since the default will only spare 4gb of memory adding memory: &quot;8GiB&quot; will spare 8 instead of 4, adding writable options below the mount points will let you write files in those locations (do it at your own risk, also mind that if writable not being set to true the files in the mounted directory become read-only) ... memory: &quot;8GiB&quot; mounts: - location: &quot;~&quot; writable: true - location: &quot;/tmp/lima&quot; writable: true ... running the command below will start your lima vm (Select Proceed with the current configuration option when prompted) $ limactl start ./singularity-ce.yml When you run the command below, you will see that your singularity-ce instance is already running $ limactl list To stop and remove/delete the instance you can use the following command respectively $ limactl stop singularity-ce $ limactl remove singularity-ce $ limactl delete singularity-ce Now we got our singularity-ce running, next step is to create the singularity container, let's say I want to create container that contains softwares from different programming language. The command below will let you enter the VM interactively $ limactl shell singularity-ce [user@lima-singularity-ce]$ You can also directly run singularity, for example the command below will run the Alpine Linux image from the Sylabs cloud library $ limactl shell singularity-ce singularity run library://alpine Since we will be creating the Singularity container image ourselves, we won't be downloading it from the Sylabs cloud library but instead we will write something called .def file. .def file or definition file is a recipe to build container image with Singularity, it's just a Singularity headers and bunch of shell commands that will be executed to create the image container. This guides from Singularity docs is very helpful in explaining what we can put in the .def file. Below is a minimal script to create a container with pandas and seaborn installed test.def Bootstrap: docker From: ubuntu:22.04 %files /Users/hariesramdhani/Documents/requirements.txt /requirements.txt %post export DEBIAN_FRONTEND=noninteractive apt-get clean all &amp;&amp; \\ apt-get update &amp;&amp; \\ apt-get upgrade -y &amp;&amp; \\ apt-get install -y \\ autoconf \\ autogen \\ build-essential \\ curl \\ libbz2-dev \\ libcurl4-openssl-dev \\ libssl-dev \\ libxml2-dev \\ zlib1g-dev \\ python3-dev \\ python3-pip pip install -r /requirements.txt /Users/hariesramdhani/Documents/requirements.txt pandas seaborn To create image from the above .def file, all you have to do is run the following command (make sure you already activated the interactive mode, notice the [user@lima-singularity-ce] before the $) [user@lima-singularity-ce]$ singularity build --fakeroot test.sif test.def The command below build the test.sif from the test.def and you either need to run it as sudo or using the --fakeroot argument. Usually it fails when I'm using sudo, this is why I'm using --fakeroot. The .sif file will appear once you successfully build it. Well it seems easy and straightforward, doesn't it? It does, but in practice, it can be more complicated when you try to build a more complex system, like for example my .def file will contain different softwares from different programming language different way to install and &quot;different&quot; everything on an ARM64 M2 Macbook. Here I am sharing the lessons I learned when building singularity image for my project. My ARM != Your AMDMac transitioned to Apple silicon which means the CPU architecture of your Macbook M series is ARM64. Why is this information important? The thing with Singularity is that it's not architecture agnostic, so when you build your singularity image file on your Mac (ARM64) and try to run it on an AMD64 HPC you will get the following error; $ singularity run test.sif FATAL: could not open image /path/to/dir/test.sif: the image's architecture (arm64) could not run on the host's (amd64) Can't we just the create the Alpine Linux on our Lima VM to be AMD64 in the first place? Yes, technically by adding the arch parameter and set it to x86_64 we will make our Lima VM architecture to be AMD64. ... arch: &quot;x86_64&quot; images: - location: &quot;https://repo.almalinux.org/almalinux/9/cloud/x86_64/images/AlmaLinux-9-GenericCloud-latest.x86_64.qcow2&quot; arch: &quot;x86_64&quot; ... This sounds straightforward and expected to be working but it didn't for me, but luckily after some hacking here and there I found a getaway! What I did for my case was to build an Ubuntu vm then install Singularity inside it. Can simply create a new instance using the default ubuntu-lts and when prompted with option don't forget to update the yaml file $ limactl start ubuntu-lts If the default editor is vim, press i to insert, edit it to add the following and esc followed w+q and enter to save ... # Change the architecture arch: &quot;x86_64&quot; # Upgrade the memory memory: &quot;8GiB&quot; mounts: # Make the mounted point writable - location: &quot;~&quot; writable: true - location: &quot;/tmp/lima&quot; writable: true ... It may take more time than the usual to create the instance (most likely because of the different architecture). Once you're done, enter the interactive mode to install singularity. $ singularity shell ubuntu-lts And run the following code to install Singularity and its dependencies, change the GO version (1.13.15) and Singularity version (v3.6.3) according to your liking, as seen in the installation guide) sudo apt-get update &amp;&amp; \\ sudo apt-get install -y build-essential \\ libseccomp-dev pkg-config squashfs-tools cryptsetup sudo rm -r /usr/local/go export VERSION=1.13.15 OS=linux ARCH=amd64 # change this as you need wget -O /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz https://dl.google.com/go/go${VERSION}.${OS}-${ARCH}.tar.gz &amp;&amp; \\ sudo tar -C /usr/local -xzf /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz echo 'export GOPATH=${HOME}/go' &gt;&gt; ~/.bashrc &amp;&amp; \\ echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' &gt;&gt; ~/.bashrc &amp;&amp; \\ source ~/.bashrc curl -sfL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | sh -s -- -b $(go env GOPATH)/bin v1.21.0 mkdir -p ${GOPATH}/src/github.com/sylabs &amp;&amp; \\ cd ${GOPATH}/src/github.com/sylabs &amp;&amp; \\ git clone https://github.com/sylabs/singularity.git &amp;&amp; \\ cd singularity git checkout v3.6.3 cd ${GOPATH}/src/github.com/sylabs/singularity &amp;&amp; \\ ./mconfig &amp;&amp; \\ cd ./builddir &amp;&amp; \\ make &amp;&amp; \\ sudo make install singularity version I tried to run the singularity image file that I built using the above VM on an AMD64 HPC and it works successfully, the building process however takes longer time than usual! NIH HPC Singularity def files are your friendWhether you want to create a cool, complex def file or need to know how to install some stuff (yes R packages I'm looking at you!), you can always refer to NIH HPC Singularity def files collection on GitHub (as there aren't many on Google search), they have it all! Just use the Github advanced search repo:NIH-HPC/singularity-def-files {keyword} to find specific type of commands, also technically you can find more by searching all over GitHub with the path:*.def &quot;Bootstrap:&quot; search keyword, but the NIH HPC was suffice for my work. The power of scriptBuilding singularity (singularity build) image file with a complex .def file may take a while (by a while I meant a whole hour) and when you run the command thousands of lines of installation progress appear (Yes R packages I'm looking at you again), troubleshooting can be very hard when you have to scroll through all of those thousands of lines and looking for where the error occurred. Fortunately we have script command on linux, script {filename_to_save_the_logs} will record all of the things that are written on the terminal whether it's an output, an error message or literally anything, it then will save it for you when you run the exit command. You can then open it using your favourite text editor and CTRL+F or command+F. Captain we need more memorySetting up the right memory is very vital as you don't want your vm to hang when building your Singularity image files. Happened several times for me before I decided to increase the memory size. Escaping R and its package dependency hellI had a very hard time installing R and some of its packages, what worked for me was to follow the recipes from NIH HPC Singularity def files collection that contain the keyword Rscript, install_github. While the installation worked it didn't solve the dependencies problem especially when installing the package using install_github and that package requires Bioconductor packages, earlier today I had a supervisory meeting with Mike (my PI) and he came up with this nice idea to actually scrape the DESCRIPTION file for each package from the Github repo and install them first before installing the package. I'll show how I do it on a separate blog post! So that concludes my writing, hopefully this can be helpful for all of the researchers that are going to the same thing.","keywords":"","version":"Next"}],"options":{"id":"default"}}